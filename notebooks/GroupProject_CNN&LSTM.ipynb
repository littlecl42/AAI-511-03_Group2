{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/littlecl42/AAI-511-03_Group2/blob/main/notebooks/GroupProject_CNN%26LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Team Project Introduction\n",
        "### Introduction\n",
        "\n",
        "Music is a form of art that is ubiquitous and has a rich history. Different composers have created music with their unique styles and compositions. However, identifying the composer of a particular piece of music can be a challenging task, especially for novice musicians or listeners. The proposed project aims to use deep learning techniques to identify the composer of a given piece of music accurately.\n",
        "\n",
        "### Objective\n",
        "\n",
        "The primary objective of this project is to develop a deep learning model that can predict the composer of a given musical score accurately. The project aims to accomplish this objective by using two deep learning techniques: Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN).\n",
        "\n",
        "### Project Timeline\n",
        "\n",
        "- Module 2 (by the end of Week 2): The course instructor will group students into teams of two to three members. Canvas, USD Email, or Slack can be used to find prospective team members.\n",
        "- Module 4 (by the end of Week 4): Each team's representative will need to submit the \"Team Project Status Update Form.\"\n",
        "- Module 7 (by the end of Week 7): Each team should submit deliverables for the course project in the final week:\n",
        "\n",
        ">1.  Project Report\n",
        ">1. Project Notebook\n",
        "\n",
        "It is critical to note that no extensions will be given for any of the final projects' due dates for any reason, and final projects submitted after the final due date will not be graded.\n",
        "Dataset\n",
        "\n",
        "The project will use a dataset consisting of musical scores from various composers. The dataset Download datasetwill contain MIDI files and sheet music of compositions from well-known classical composers like Bach, Beethoven, Chopin, Mozart, Schubert, etc. The dataset should be labeled with the name of the composer for each score.\n",
        "\n",
        "### Dataset\n",
        "The project will use a dataset consisting of musical scores from various composers. Download the dataset from Kaggle websiteLinks to an external site..\n",
        "\n",
        "The dataset contains the midi files of compositions from well-known classical composers like Bach, Beethoven, Chopin, and Mozart. The dataset should be labeled with the name of the composer for each score. Please only do your prediction only for below composers, therefore you need to select the required composers from the given dataset above.\n",
        "\n",
        ">1. Bach\n",
        ">1. Beethoven\n",
        ">1. Chopin\n",
        ">1. Mozart\n",
        "\n",
        "### Methodology\n",
        "\n",
        "The proposed project will be implemented using the following steps:\n",
        "\n",
        "1. Data Collection: Data is collected and provided to you.\n",
        "1. Data Pre-processing: Convert the musical scores into a format suitable for deep learning models. This involves converting the musical scores into MIDI files and applying data augmentation techniques.\n",
        "1. Feature Extraction: Extract features from the MIDI files, such as notes, chords, and tempo, using music analysis tools.\n",
        "1. Model Building: Develop a deep learning model using LSTM and CNN architectures to classify the musical scores according to the composer.\n",
        "1. Model Training: Train the deep learning model using the pre-processed and feature-extracted data.\n",
        "1. Model Evaluation: Evaluate the performance of the deep learning model using accuracy, precision, and recall metrics.\n",
        "1. Model Optimization: Optimize the deep learning model by fine-tuning hyperparameters.\n",
        "\n",
        "### Deliverables\n",
        "\n",
        "1. Project Report: A comprehensive documentation/report that describes the methodology, data pre-processing steps, feature extraction techniques, model architecture, and training process for reproducibility and future reference. Write your technical report in APA 7 style (here is a Sample Professional Paper format to follow). Please submit the report in PDF format and use the File naming convention DeliverableName-TeamNumber.pdf; for example, Project_Report-Team1.pdf\n",
        "\n",
        "Your report should:\n",
        "contain a reference list that includes any external sources, libraries, or frameworks used during the project, including proper citations or acknowledgments.\n",
        "\n",
        "include a concluding section or markdown cell that summarizes the project, highlights key findings, and suggests any potential future improvements or extensions to the work.\n",
        "\n",
        "2. Project Notebook: A Jupyter Notebook file (.ipynb) that contains the entire project code, including data pre-processing, feature extraction, model building, training, evaluation, and any additional analysis or visualizations performed during the project.\n",
        "\n",
        "This deliverable will be exported from a Jupyter Notebook and submitted as a PDF or HTML file.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The proposed project aims to use deep learning techniques to accurately predict the composer of a given musical score. The project will be implemented using LSTM and CNN architectures and will involve data pre-processing, feature extraction, model building, training, and evaluation. The final model can be used by novice musicians, listeners, and music enthusiasts to identify the composer of a musical piece accurately.\n",
        "\n",
        "### Power Usage for this Project\n",
        "\n",
        "You can use Google Colab GPU and TPU in case you need more computation power. Change your runtime in Google Colab notebook to GPU or TPU.\n",
        "Another option is to buy the subscription in case you need more computational power (recommended).\n",
        "\n",
        "Please follow this link to do so: Google Colab Pro+.\n",
        "NOTE: Team members may not get the same grade on the Final Team Project, depending on each team member's level of contribution.\n",
        "\n",
        "To understand how your work will be assessed, view the assignment rubric on the Final Team Project page.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bix0Kn-bTtNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretty_midi\n"
      ],
      "metadata": {
        "id": "f8z9HA5Fbpjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libs for the project\n",
        "import kagglehub\n",
        "import pretty_midi\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import re\n",
        "import difflib\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "bkkAxNGqbNmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "midi_path = path"
      ],
      "metadata": {
        "id": "JuFW7u5unC8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "known_composers = [\n",
        "    \"Bach\", \"Mozart\", \"Beethoven\", \"Chopin\", \"Tchaikovsky\", \"Handel\", \"Schubert\",\n",
        "    \"Haydn\", \"Brahms\", \"Liszt\", \"Mendelssohn\", \"Debussy\", \"Ravel\", \"Grieg\", \"Dvorak\",\n",
        "    \"Vivaldi\", \"Stravinsky\", \"Rachmaninoff\", \"Mahler\", \"Shostakovich\", \"Alkan\", \"Albeniz\",\n",
        "    \"Ambroise\", \"Arensky\", \"Arndt\", \"Bacewitz\"\n",
        "]\n",
        "\n",
        "def load_midi_files(directory):\n",
        "    midi_data = []\n",
        "    bad_files = []  # Track bad files\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith((\".mid\", \".midi\")):\n",
        "                try:\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    midi = pretty_midi.PrettyMIDI(full_path)\n",
        "                    midi_data.append((full_path, file, midi))\n",
        "                except Exception:\n",
        "                    bad_files.append(file)\n",
        "                    continue\n",
        "    return midi_data, bad_files"
      ],
      "metadata": {
        "id": "reOXALSYnDA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_composer_from_path(full_path):\n",
        "    parts = os.path.normpath(full_path).split(os.sep)\n",
        "    for part in reversed(parts[:-1]):  # Exclude filename\n",
        "        for composer in known_composers:\n",
        "            if composer.lower() in part.lower():\n",
        "                return composer\n",
        "    return \"Unknown\""
      ],
      "metadata": {
        "id": "LfwnZwmjnDQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(midi_dataset):\n",
        "    rows = []\n",
        "    for full_path, filename, midi in midi_dataset:\n",
        "\n",
        "        try:\n",
        "            pitches = []\n",
        "            durations = []\n",
        "            for instrument in midi.instruments:\n",
        "                for note in instrument.notes:\n",
        "                    pitches.append(note.pitch)\n",
        "                    durations.append(note.end - note.start)\n",
        "            if pitches:\n",
        "                pitch_mean = np.mean(pitches)\n",
        "                pitch_std = np.std(pitches)\n",
        "                pitch_min = np.min(pitches)\n",
        "                pitch_max = np.max(pitches)\n",
        "                duration_mean = np.mean(durations)\n",
        "                duration_std = np.std(durations)\n",
        "                duration_min = np.min(durations)\n",
        "                duration_max = np.max(durations)\n",
        "            else:\n",
        "                pitch_mean = pitch_std = pitch_min = pitch_max = 0\n",
        "                duration_mean = duration_std = duration_min = duration_max = 0\n",
        "\n",
        "            duration = midi.get_end_time()\n",
        "            tempi = midi.get_tempo_changes()[1]\n",
        "\n",
        "            if len(tempi) > 0:\n",
        "                tempo_mean = np.mean(tempi)\n",
        "                tempo_std = np.std(tempi)\n",
        "            else:\n",
        "                tempo_mean = tempo_std = 0\n",
        "\n",
        "            key_number = midi.key_signature_changes[0].key_number if midi.key_signature_changes else -1\n",
        "\n",
        "            duration = midi.get_end_time()\n",
        "            num_instruments = len(midi.instruments)\n",
        "\n",
        "            tempo_changes = midi.get_tempo_changes()\n",
        "            tempo = float(np.mean(tempo_changes[1])) if len(tempo_changes[1]) > 0 else np.nan\n",
        "\n",
        "            notes = [note.pitch for instrument in midi.instruments for note in instrument.notes if not instrument.is_drum]\n",
        "            avg_pitch = np.mean(notes) if notes else np.nan\n",
        "            note_density = len(notes) / duration if duration > 0 else 0\n",
        "\n",
        "            composer = extract_composer_from_path(full_path)\n",
        "\n",
        "            rows.append({\n",
        "                'filename': filename,\n",
        "                'composer': composer,\n",
        "                'duration': duration,\n",
        "                'num_instruments': num_instruments,\n",
        "                'note_count': len(pitches),\n",
        "                'note_density': note_density,\n",
        "                'notes': notes,\n",
        "                'pitch_mean': pitch_mean,\n",
        "                'pitch_std': pitch_std,\n",
        "                'pitch_min': pitch_min,\n",
        "                'pitch_max': pitch_max,\n",
        "                'avg_pitch': avg_pitch,\n",
        "                'duration_mean': duration_mean,\n",
        "                'duration_std': duration_std,\n",
        "                'duration_min': duration_min,\n",
        "                'duration_max': duration_max,\n",
        "                'duration': duration,\n",
        "                'num_instruments': num_instruments,\n",
        "                'tempo_mean': tempo_mean,\n",
        "                'tempo_std': tempo_std,\n",
        "                'tempo': tempo,\n",
        "                'key': key_number\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "zOrkuvJshr4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "midi_dataset, bad_files = load_midi_files(midi_path)\n",
        "print(f\"Loaded {len(midi_dataset)} good MIDI files.\")\n",
        "print(f\"Skipped {len(bad_files)} broken files.\")"
      ],
      "metadata": {
        "id": "5s9Ch3sdlile",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = extract_features(midi_dataset)"
      ],
      "metadata": {
        "id": "BBB5ZaeDllX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "composer_counts = df[\"composer\"].value_counts()\n",
        "print(composer_counts)"
      ],
      "metadata": {
        "id": "BnCQHZj6g74y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the selected composers\n",
        "df = df[df[\"composer\"].isin([\"Bach\", \"Beethoven\", \"Chopin\", \"Mozart\"])].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "m5kUREWRnnvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "composer_counts = df[\"composer\"].value_counts()\n",
        "print(composer_counts)"
      ],
      "metadata": {
        "id": "rah_cPpioCbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "W-6PIa2KoeJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show basic summary of the data\n",
        "df.info()\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "vzmBZqXnog5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of MIDI files per composer\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(data=df, x=\"composer\",  hue='composer', palette='Set2', legend=False)\n",
        "plt.title(\"Number of MIDI Files per Composer\")\n",
        "plt.xlabel(\"Composer\")\n",
        "plt.ylabel(\"Number of Files\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0YU6wYarQJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Duration by Composer\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.boxplot(data=df, x=\"composer\", y=\"duration\", hue='composer', palette='Set2', legend=False)\n",
        "plt.title(\"Distribution of Duration by Composer\")\n",
        "plt.xlabel(\"Composer\")\n",
        "plt.ylabel(\"Duration (seconds)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ttoyr7y4rX0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Instruments per Composer\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.violinplot(data=df, x=\"composer\", y=\"num_instruments\", inner=\"quartile\", hue='composer', palette='Set2', legend=False)\n",
        "plt.title(\"Number of Instruments per Composer\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TQvvvUQarfjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note Density (notes/sec) by Composer\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.boxplot(data=df, x='composer', y='note_density', hue='composer', palette='Set2', legend=False)\n",
        "plt.title(\"Note Density (notes/sec) by Composer\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N4cCrYZDrpFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Corrleation Metrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df.drop(columns=[\"filename\"]).corr(numeric_only=True), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g3D-Fry9sbql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How multiple features relate to each other by composer?\n",
        "sns.pairplot(df, hue=\"composer\", vars=[\"duration\", \"tempo\", \"avg_pitch\", \"note_density\"])\n",
        "plt.suptitle(\"Pairwise Feature Distributions by Composer\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ovt6ANs3sqfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sequence Length Distribution\n",
        "sequence_lengths = df['notes'].apply(len)\n",
        "plt.hist(sequence_lengths, bins=30)\n",
        "plt.title('Sequence Length Distribution')\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ilBpmOX1SoCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pitch Distribution\n",
        "all_pitches = [pitch for seq in df['notes'] for pitch in seq]\n",
        "sns.histplot(all_pitches, bins=50)\n",
        "plt.title('Pitch Distribution')"
      ],
      "metadata": {
        "id": "GOP054W6SoGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Composer-wise Note Count\n",
        "df['note_count'] = df['notes'].apply(len)\n",
        "sns.boxplot(data=df, x='composer', y='note_count', hue='composer', palette='Set2', legend=False)"
      ],
      "metadata": {
        "id": "139djg5ySoJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Top N Most Frequent Pitches\n",
        "from collections import Counter\n",
        "Counter(all_pitches).most_common(10)"
      ],
      "metadata": {
        "id": "ZQZrU8JLSoNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature means + std devs for each composer:\n",
        "summary_stats = df.groupby(\"composer\")[[\"duration\", \"tempo\", \"avg_pitch\", \"note_density\"]].agg(['mean', 'std'])\n",
        "print(summary_stats)\n"
      ],
      "metadata": {
        "id": "RyXfA9XTtQ18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "IJIRspmXnDgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN & LSTM Hybrid Model"
      ],
      "metadata": {
        "id": "n7JzFdfzeX-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare data arrays\n",
        "# Pad the sequences with zeros to a uniform length\n",
        "max_len = df['notes'].apply(len).max()\n",
        "padded_notes = df['notes'].apply(lambda x: x + [0] * (max_len - len(x)))\n",
        "\n",
        "X = np.stack(padded_notes.values)\n",
        "y_labels = df['composer'].values"
      ],
      "metadata": {
        "id": "nAjPs5U8cRFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y_labels)"
      ],
      "metadata": {
        "id": "5NZDCnQfcRIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded,\n",
        "    test_size=0.2,\n",
        "    stratify=y_encoded,\n",
        "    random_state=42\n",
        ")\n",
        "# Add channel dimension for CNN\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)"
      ],
      "metadata": {
        "id": "WPdWMtTxcRLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the Hybrid CNN + LSTM Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "input_length = X_train.shape[1]\n",
        "vocab_size = len(note2idx) + 1  # for Embedding layer\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(input_length, 1)),\n",
        "    layers.Reshape((input_length, 1)),\n",
        "\n",
        "    # CNN for local feature extraction\n",
        "    layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
        "    layers.MaxPooling1D(pool_size=2),\n",
        "    layers.Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
        "    layers.MaxPooling1D(pool_size=2),\n",
        "\n",
        "    # Flattened CNN output to LSTM\n",
        "    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
        "    layers.LSTM(64),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(len(le.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_2TtR1tjcRN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the model\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=0\n",
        ")"
      ],
      "metadata": {
        "id": "2ZMVXCOgcRQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluate the model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.legend()\n",
        "plt.title('Training History')\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(X_test).argmax(axis=1)\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TiZGQ_w5cRTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LSTM model demonstrates moderate performance in classifying musical compositions by composer, achieving an overall accuracy of 70%. As shown in the training history, the model converges steadily, with training accuracy reaching approximately 75% and validation accuracy peaking near 69%, though it shows some instability, suggesting mild overfitting. The model performs best on Bach, with a high precision (0.81) and recall (0.94), indicating strong recognition of his compositional style. However, performance significantly drops for Beethoven, Chopin, and Mozart, with low recall and F1-scores (â‰¤ 0.51), implying difficulty in distinguishing among these composers. The confusion matrix confirms this imbalance, as many non-Bach samples are misclassified as Bach. Overall, while the LSTM captures temporal patterns well for Bach, it lacks discriminative power across more stylistically similar composers, indicating a need for improved feature representation, class balancing, or more complex architectures.\n",
        "\n"
      ],
      "metadata": {
        "id": "ohxv5qgPQrSD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dz8d7YQUVWgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XK7IPT7eVWoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xrr2aSkkpHIa"
      }
    }
  ]
}